{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Noun-Adjective Pairs Sorted by Review Count:\n",
      "Pair: ('알', '있는'), Review Count: 795\n",
      "Pair: ('볼', '있는'), Review Count: 599\n",
      "Pair: ('살', '있는'), Review Count: 563\n",
      "Pair: ('게', '아니라'), Review Count: 518\n",
      "Pair: ('볼', '있어'), Review Count: 492\n",
      "Pair: ('알', '있어서'), Review Count: 304\n",
      "Pair: ('볼', '있어서'), Review Count: 273\n",
      "Pair: ('관심', '있는'), Review Count: 265\n",
      "Pair: ('보고', '있습니다'), Review Count: 252\n",
      "Pair: ('가지', '있는'), Review Count: 243\n",
      "Pair: ('가치', '있는'), Review Count: 224\n",
      "Pair: ('보고', '있어요'), Review Count: 211\n",
      "Pair: ('의미', '있는'), Review Count: 199\n",
      "Pair: ('깊이', '있는'), Review Count: 193\n",
      "Pair: ('볼', '있게'), Review Count: 187\n",
      "Pair: ('꼭', '필요한'), Review Count: 160\n",
      "Pair: ('가장', '좋은'), Review Count: 113\n",
      "Pair: ('아주', '좋은'), Review Count: 107\n",
      "Pair: ('수가', '없다'), Review Count: 95\n",
      "Pair: ('깊이', '있게'), Review Count: 84\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from konlpy.tag import Okt\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# 데이터 로드\n",
    "data = pd.read_csv(\"merged_data_with_categories.csv\")\n",
    "\n",
    "# Okt 형태소 분석기 초기화\n",
    "okt = Okt()\n",
    "\n",
    "# 리뷰 데이터 전처리: \"리뷰 없음\" 제거\n",
    "data = data[data[\"review_content\"] != \"리뷰 없음\"]\n",
    "\n",
    "# 리뷰 데이터에서 중복 제거\n",
    "data = data.drop_duplicates(subset=[\"review_content\"])\n",
    "\n",
    "# 불용어 리스트 정의\n",
    "stopwords = [\n",
    "    \"의\", \"가\", \"이\", \"은\", \"들\", \"는\", \"좀\", \"잘\", \"걍\", \"과\", \"도\", \"를\", \"으로\", \"자\", \"에\", \"와\",\n",
    "    \"한\", \"하다\", \"것\", \"라고\", \"에게\", \"라면\", \"을\", \"이라\", \"라니\", \"있다\", \"아\", \"랑\", \"쯤된\", \"에서\",\n",
    "    \"에선\", \"어\", \"이지만\", \"으로나\", \"때\", \"때는\", \"때라면\", \"때라서\", \"라\", \"이다\", \"있\", \"죠\", \"고\",\n",
    "    \"니\", \"로\", \"같\", \"어서\", \"어요\", \"는데\", \"습니다\", \"면서\", \"많이\", \"마\", \"더\", \"그렇다\", \"당\", \"안\",\n",
    "    \"정말\", \"같다\", \"임\", \"만\", \"인\", \"부터\", \"저\", \"우리\", \"너\", \"저희\", \"그\", \"수\", \"아무\", \"나\",\n",
    "    \"너희\", \"제\", \"책\", \"리뷰\", \"배송\", \"소설\", \"좋다\", \"작품\", \"뿐\", \"거\", \"중\", \"입니다\", \"추합니다\",\n",
    "]\n",
    "\n",
    "# 리뷰 데이터 전처리 함수\n",
    "def preprocess_review(review):\n",
    "    review = re.sub(r\"[^가-힣\\s]\", \"\", str(review))  # 한글과 공백만 남기기\n",
    "    return review\n",
    "\n",
    "# 명사-형용사 조합 추출 함수\n",
    "def extract_noun_adj_pairs(text):\n",
    "    tokens = okt.pos(text)  # 형태소 및 품사 태깅\n",
    "    filtered_tokens = [\n",
    "        (word, tag) for word, tag in tokens if word not in stopwords\n",
    "    ]  # 불용어 제거\n",
    "    pairs = []\n",
    "    for i in range(len(filtered_tokens) - 1):\n",
    "        if filtered_tokens[i][1] == \"Noun\" and filtered_tokens[i + 1][1] == \"Adjective\":\n",
    "            pairs.append((filtered_tokens[i][0], filtered_tokens[i + 1][0]))\n",
    "    return pairs\n",
    "\n",
    "# 데이터 전처리 및 명사-형용사 조합 추출\n",
    "data[\"cleaned_review\"] = data[\"review_content\"].apply(preprocess_review)\n",
    "data[\"noun_adj_pairs\"] = data[\"cleaned_review\"].apply(extract_noun_adj_pairs)\n",
    "\n",
    "# 모든 리뷰에서 추출된 명사-형용사 조합 리스트 생성\n",
    "all_pairs = [pair for sublist in data[\"noun_adj_pairs\"] for pair in sublist]\n",
    "\n",
    "# 명사-형용사 조합 빈도 계산\n",
    "pair_counts = Counter(all_pairs)\n",
    "\n",
    "# 자주 사용된 조합 추출 (상위 20개)\n",
    "common_pairs = pair_counts.most_common(20)\n",
    "\n",
    "# 조합별 리뷰와 리뷰 개수 추출\n",
    "pair_reviews = []\n",
    "for pair, _ in common_pairs:\n",
    "    pair_pattern = f\"{pair[0]}.*{pair[1]}\"  # 명사와 형용사가 연속적으로 등장하는 패턴\n",
    "    matching_reviews = data[data[\"cleaned_review\"].str.contains(pair_pattern, na=False)]\n",
    "    pair_reviews.append((pair, len(matching_reviews)))\n",
    "\n",
    "# 리뷰 개수로 내림차순 정렬\n",
    "sorted_pair_reviews = sorted(pair_reviews, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# 출력\n",
    "print(\"Top Noun-Adjective Pairs Sorted by Review Count:\")\n",
    "for pair, review_count in sorted_pair_reviews:\n",
    "    print(f\"Pair: {pair}, Review Count: {review_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Adjective-Noun Pairs Sorted by Review Count:\n",
      "Pair: ('많은', '도움'), Count: 389\n",
      "Pair: ('같은', '느낌'), Count: 227\n",
      "Pair: ('있는', '사람'), Count: 212\n",
      "Pair: ('많은', '사람'), Count: 196\n",
      "Pair: ('많은', '생각'), Count: 193\n",
      "Pair: ('좋은', '글'), Count: 154\n",
      "Pair: ('좋아하는', '사람'), Count: 134\n",
      "Pair: ('좋은', '내용'), Count: 133\n",
      "Pair: ('아름다', '움'), Count: 131\n",
      "Pair: ('빨간', '머리'), Count: 128\n",
      "Pair: ('있는', '내용'), Count: 115\n",
      "Pair: ('많은', '분'), Count: 107\n",
      "Pair: ('좋아하는', '작가'), Count: 106\n",
      "Pair: ('있다고', '생각'), Count: 103\n",
      "Pair: ('있는', '시간'), Count: 94\n",
      "Pair: ('미야', '베'), Count: 94\n",
      "Pair: ('좋을', '듯'), Count: 91\n",
      "Pair: ('있는', '기회'), Count: 91\n",
      "Pair: ('자세한', '설명'), Count: 83\n",
      "Pair: ('아쉬운', '점'), Count: 81\n"
     ]
    }
   ],
   "source": [
    "# 형용사-명사 조합 추출 함수\n",
    "def extract_adj_noun_pairs(text):\n",
    "    tokens = okt.pos(text)  # 형태소 및 품사 태깅\n",
    "    filtered_tokens = [\n",
    "        (word, tag) for word, tag in tokens if word not in stopwords\n",
    "    ]  # 불용어 제거\n",
    "    pairs = []\n",
    "    for i in range(len(filtered_tokens) - 1):\n",
    "        if filtered_tokens[i][1] == \"Adjective\" and filtered_tokens[i + 1][1] == \"Noun\":\n",
    "            pairs.append((filtered_tokens[i][0], filtered_tokens[i + 1][0]))\n",
    "    return pairs\n",
    "\n",
    "# 데이터 전처리 및 형용사-명사 조합 추출\n",
    "data[\"cleaned_review\"] = data[\"review_content\"].apply(preprocess_review)\n",
    "data[\"adj_noun_pairs\"] = data[\"cleaned_review\"].apply(extract_adj_noun_pairs)\n",
    "\n",
    "# 모든 리뷰에서 추출된 형용사-명사 조합 리스트 생성\n",
    "all_adj_noun_pairs = [pair for sublist in data[\"adj_noun_pairs\"] for pair in sublist]\n",
    "\n",
    "# 형용사-명사 조합 빈도 계산\n",
    "pair_counts = Counter(all_adj_noun_pairs)\n",
    "\n",
    "# 자주 사용된 조합 추출 (상위 20개)\n",
    "common_pairs = pair_counts.most_common(20)\n",
    "\n",
    "# 출력\n",
    "print(\"Top Adjective-Noun Pairs Sorted by Review Count:\")\n",
    "for pair, count in common_pairs:\n",
    "    print(f\"Pair: {pair}, Count: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Noun-Noun Pairs Sorted by Review Count:\n",
      "Pair: ('하나', '하나'), Count: 446\n",
      "Pair: ('바로', '구매'), Count: 427\n",
      "Pair: ('다시', '한번'), Count: 400\n",
      "Pair: ('그림', '체'), Count: 318\n",
      "Pair: ('소장', '가치'), Count: 287\n",
      "Pair: ('스마트', '스토어'), Count: 269\n",
      "Pair: ('중간', '중간'), Count: 262\n",
      "Pair: ('대한', '이야기'), Count: 225\n",
      "Pair: ('처음', '접'), Count: 199\n",
      "Pair: ('펀딩', '참여'), Count: 191\n",
      "Pair: ('또', '다른'), Count: 187\n",
      "Pair: ('영어', '공부'), Count: 180\n",
      "Pair: ('다음', '권'), Count: 171\n",
      "Pair: ('이번', '권'), Count: 166\n",
      "Pair: ('강력', '추천'), Count: 162\n",
      "Pair: ('머리', '앤'), Count: 161\n",
      "Pair: ('북', '펀드'), Count: 160\n",
      "Pair: ('영화', '보고'), Count: 155\n",
      "Pair: ('대해', '생각'), Count: 152\n",
      "Pair: ('마음', '듭니'), Count: 148\n"
     ]
    }
   ],
   "source": [
    "# 명사-명사 조합 추출 함수\n",
    "def extract_noun_noun_pairs(text):\n",
    "    tokens = okt.pos(text)  # 형태소 및 품사 태깅\n",
    "    filtered_tokens = [\n",
    "        (word, tag) for word, tag in tokens if word not in stopwords\n",
    "    ]  # 불용어 제거\n",
    "    pairs = []\n",
    "    for i in range(len(filtered_tokens) - 1):\n",
    "        if filtered_tokens[i][1] == \"Noun\" and filtered_tokens[i + 1][1] == \"Noun\":\n",
    "            pairs.append((filtered_tokens[i][0], filtered_tokens[i + 1][0]))\n",
    "    return pairs\n",
    "\n",
    "# 데이터 전처리 및 명사-명사 조합 추출\n",
    "data[\"cleaned_review\"] = data[\"review_content\"].apply(preprocess_review)\n",
    "data[\"noun_noun_pairs\"] = data[\"cleaned_review\"].apply(extract_noun_noun_pairs)\n",
    "\n",
    "# 모든 리뷰에서 추출된 명사-명사 조합 리스트 생성\n",
    "all_noun_noun_pairs = [pair for sublist in data[\"noun_noun_pairs\"] for pair in sublist]\n",
    "\n",
    "# 명사-명사 조합 빈도 계산\n",
    "pair_counts = Counter(all_noun_noun_pairs)\n",
    "\n",
    "# 자주 사용된 조합 추출 (상위 20개)\n",
    "common_pairs = pair_counts.most_common(20)\n",
    "\n",
    "# 출력\n",
    "print(\"Top Noun-Noun Pairs Sorted by Review Count:\")\n",
    "for pair, count in common_pairs:\n",
    "    print(f\"Pair: {pair}, Count: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Keywords per Cluster:\n",
      "Cluster 0: 사람 이해, 생각 다른, 사람 추천, 사람 생각, 다른 사람\n",
      "Cluster 1: 소장 가치, 많은 도움, 다시 한번, 좋은 입니다, 바로 구매\n",
      "Cluster 2: 소장 가치, 소장 가치 있는, 가치 있는, 가치 있는 입니다, 있는 입니다\n",
      "Cluster 3: 생각 듭니, 있어서 좋았습니다, 정말 생각, 가독성 좋고, 중간 중간\n",
      "Cluster 4: 선물 좋은, 선물 주문, 선물 구입, 선물 구매, 친구 선물\n",
      "Cluster 5: 초등 학년, 추천 추천, 소식 바로, 보고 바로, 바로 주문\n",
      "Cluster 6: 한번 야할, 누구 한번, 생각 지금, 초등 학년, 우리 아이\n",
      "Cluster 7: 생각 좋은, 생각 거리, 생각 입니다, 대해 많은, 많은 생각\n",
      "Cluster 8: 구입 정말, 추천 입니다, 내용 정말, 있어서 정말, 정말 좋아요\n",
      "Cluster 9: 이야기 하나, 스마트 스토어, 문장 하나 하나, 문장 하나, 하나 하나\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "# 1. 데이터 로드 및 전처리\n",
    "data = pd.read_csv(\"merged_data_without_duplicates.csv\")\n",
    "data = data[data[\"review_content\"] != \"리뷰 없음\"]  # \"리뷰 없음\" 제거\n",
    "\n",
    "# 중복된 리뷰 내용 제거\n",
    "data = data.drop_duplicates(subset=[\"review_content\"]).reset_index(drop=True)\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "# 2. 형태소 분석을 통한 키워드 추출\n",
    "def extract_keywords(text):\n",
    "    tokens = okt.pos(text)\n",
    "    keywords = [word for word, tag in tokens if tag in [\"Noun\", \"Adjective\"]]\n",
    "    return \" \".join(keywords)\n",
    "\n",
    "data[\"processed_review\"] = data[\"review_content\"].apply(extract_keywords)\n",
    "\n",
    "# 3. TF-IDF 계산\n",
    "vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(2, 3))\n",
    "tfidf_matrix = vectorizer.fit_transform(data[\"processed_review\"])\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "# 4. KMeans 클러스터링\n",
    "kmeans = KMeans(n_clusters=10, random_state=0).fit(tfidf_matrix)\n",
    "data[\"cluster\"] = kmeans.labels_\n",
    "\n",
    "# 5. 각 클러스터에서 대표 키워드 추출\n",
    "def get_top_keywords(cluster, n_terms=5):\n",
    "    cluster_indices = data[data[\"cluster\"] == cluster].index\n",
    "    cluster_tfidf = tfidf_matrix[cluster_indices.tolist()].toarray()\n",
    "    mean_tfidf = cluster_tfidf.mean(axis=0)\n",
    "    top_terms = [terms[i] for i in mean_tfidf.argsort()[-n_terms:]]\n",
    "    return top_terms\n",
    "\n",
    "top_keywords_per_cluster = {i: get_top_keywords(i) for i in range(kmeans.n_clusters)}\n",
    "\n",
    "# 6. 결과 출력\n",
    "print(\"Top Keywords per Cluster:\")\n",
    "for cluster, keywords in top_keywords_per_cluster.items():\n",
    "    print(f\"Cluster {cluster}: {', '.join(keywords)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Nouns by TF-IDF (After Expanded Stopword Removal):\n",
      "Keyword: 작가, Score: 1741.11\n",
      "Keyword: 이야기, Score: 1684.64\n",
      "Keyword: 기대, Score: 1541.13\n",
      "Keyword: 최고, Score: 837.48\n",
      "Keyword: 우리, Score: 836.57\n",
      "Keyword: 사랑, Score: 829.84\n",
      "Keyword: 구입, Score: 816.59\n",
      "Keyword: 도움, Score: 815.99\n",
      "Keyword: 역시, Score: 803.53\n",
      "Keyword: 하나, Score: 787.04\n",
      "Keyword: 시간, Score: 773.18\n",
      "Keyword: 처음, Score: 771.41\n",
      "Keyword: 다른, Score: 755.19\n",
      "Keyword: 대해, Score: 751.21\n",
      "Keyword: 한번, Score: 747.92\n",
      "Keyword: 설명, Score: 730.29\n",
      "Keyword: 부분, Score: 724.75\n",
      "Keyword: 이번, Score: 685.18\n",
      "Keyword: 영화, Score: 679.41\n",
      "Keyword: 다음, Score: 678.46\n",
      "Top nouns saved to top_nouns_tfidf_filtered.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from konlpy.tag import Okt\n",
    "import re\n",
    "\n",
    "# 1. 데이터 로드 및 전처리\n",
    "data = pd.read_csv(\"merged_data_without_duplicates.csv\")\n",
    "data = data[data[\"review_content\"] != \"리뷰 없음\"]  # \"리뷰 없음\" 제거\n",
    "\n",
    "# 중복된 리뷰 제거\n",
    "data = data.drop_duplicates(subset=[\"review_content\"]).reset_index(drop=True)\n",
    "\n",
    "# 2. Okt 초기화\n",
    "okt = Okt()\n",
    "\n",
    "# 3. 확장된 불용어 리스트 정의\n",
    "stopwords = [\n",
    "    # 기본 불용어\n",
    "    \"책\", \"내용\", \"저\", \"것\", \"거\", \"수\", \"때\", \"듯\", \"너무\", \"진짜\",\n",
    "    \"정말\", \"그\", \"이\", \"그리고\", \"제\", \"더\", \"좀\", \"이런\", \"저런\",\n",
    "    \"많이\", \"많은\", \"사람\", \"대한\", \"좋은\", \"같다\", \"보고\", \"많다\", \n",
    "    \"좋다\", \"하는\", \"해서\", \"함\", \"이다\", \"입니다\", \"하는데\", \"하는거\", \n",
    "    \"라서\", \"로\", \"에서\", \"이라고\", \"로서\", \"라며\",\n",
    "\n",
    "    # 추가된 불용어 (결과에서 발견된 단어들)\n",
    "    \"생각\", \"구매\", \"정말\", \"사람\", \"마음\", \"소설\", \"추천\", \"작품\", \n",
    "    \"다시\", \"번역\", \"대한\", \"느낌\", \"이해\", \"그림\", \"공부\"\n",
    "]\n",
    "\n",
    "# 4. 명사 추출 함수 정의 (불용어 제거 포함)\n",
    "def extract_nouns(text):\n",
    "    # 한글 전처리: 특수 문자 제거\n",
    "    text = re.sub(r\"[^가-힣\\s]\", \"\", str(text))\n",
    "    # 명사 추출\n",
    "    nouns = okt.nouns(text)\n",
    "    # 불용어 제거\n",
    "    filtered_nouns = [word for word in nouns if word not in stopwords and len(word) > 1]\n",
    "    return \" \".join(filtered_nouns)\n",
    "\n",
    "# 모든 리뷰에서 명사만 추출\n",
    "data[\"processed_review\"] = data[\"review_content\"].apply(extract_nouns)\n",
    "\n",
    "# 5. TF-IDF 계산\n",
    "vectorizer = TfidfVectorizer(max_features=1000)  # 상위 1000개 명사\n",
    "tfidf_matrix = vectorizer.fit_transform(data[\"processed_review\"])\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "# 6. 명사 키워드의 TF-IDF 점수 계산\n",
    "tfidf_scores = tfidf_matrix.sum(axis=0).A1  # TF-IDF 점수 합산\n",
    "keywords = sorted(\n",
    "    zip(terms, tfidf_scores), key=lambda x: x[1], reverse=True\n",
    ")  # 점수 기준 내림차순 정렬\n",
    "\n",
    "# 7. 주요 명사 키워드 출력\n",
    "top_n_keywords = 20  # 상위 20개 키워드\n",
    "print(\"Top Nouns by TF-IDF (After Expanded Stopword Removal):\")\n",
    "for keyword, score in keywords[:top_n_keywords]:\n",
    "    print(f\"Keyword: {keyword}, Score: {score:.2f}\")\n",
    "\n",
    "# 8. CSV 파일로 저장 (옵션)\n",
    "output_file = \"top_nouns_tfidf_filtered.csv\"\n",
    "pd.DataFrame(keywords, columns=[\"Keyword\", \"Score\"]).to_csv(output_file, index=False)\n",
    "print(f\"Top nouns saved to {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
